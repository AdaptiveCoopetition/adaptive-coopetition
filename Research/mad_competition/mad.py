import re
from dataclasses import dataclass
from typing import Dict, List
from mad_util import *

from autogen_core import (
    AgentId,
    DefaultTopicId,
    MessageContext,
    RoutedAgent,
    SingleThreadedAgentRuntime,
    TypeSubscription,
    default_subscription,
    message_handler,
)
from autogen_core.models import (
    AssistantMessage,
    ChatCompletionClient,
    LLMMessage,
    SystemMessage,
    UserMessage,
)
from autogen_ext.models.openai import OpenAIChatCompletionClient

###################################################################
#  Communication
###################################################################
@dataclass
class Question:
    content: str
    index: str


@dataclass
class Answer:
    content: str


@dataclass
class SolverRequest:
    content: str
    question: str
    do_reset: bool


@dataclass
class IntermediateSolverResponse:
    content: str
    question: str
    answer: str
    round: int


# Critique Request for Competition
# partial_solution: text generated by the LLM, math solving reasoning and process
# question: question currently being solved by the agent
# answer: current final numerical answer from the LLM
# round: round number
# requester: the agent that requested the critique (the same as the agent who is receiving the critique)
@dataclass
class CritiqueRequest:
    content: str
    question: str
    answer: str
    round: int
    requester: str

# Critique Response for Competition
# critique: critique of previous partial solution, to be used to refine partial solution
# partial_solution: text generated by the LLM, math solving reasoning and process
# question: question currently being solved by the agent
# answer: final numerical answer from the LLM
# round: round number
@dataclass
class CritiqueResponse:
    critique: str
    content: str
    question: str 
    answer: str
    round: int


@dataclass
class FinalSolverResponse:
    solver: str
    answer: str

###################################################################
#  Math Solver
###################################################################
@default_subscription
class MathSolver(RoutedAgent):
    def __init__(self, model_client: ChatCompletionClient, topic_type: str, num_neighbors: int, max_round: int, logger) -> None:
        super().__init__("A debator.")
        self._logger = logger
        self._topic_type = topic_type
        self._model_client = model_client
        self._num_neighbors = num_neighbors
        self._history: List[LLMMessage] = []
        self._buffer: Dict[int, List[IntermediateSolverResponse]] = {}
        self._system_messages = [
            SystemMessage(
                content=("""
Your task is to assist in solving a math reasoning problem step by step.

# Output format

- Add only one step per response.
- Clearly explain your reasoning.
- If reaching the final answer, use the format: The answer is #### [numerical answer].
- Limit your response to 100 words.
""")
            )
        ]
        self._round = 0
        self._max_round = max_round
        self._logger.log(f"mad.py MathSolver {self.id} init topic_type={self._topic_type}")

    def reset(self) -> None:
        self._round = 0
        self._history.clear()
        self._buffer.clear()

# for competition: initial attempt at the first step of the question; no critiques from peers/no history
    @message_handler
    async def first_step(self, message: SolverRequest, ctx: MessageContext) -> None:
        self.reset()
        self._logger.log(f"MathSolver {self.id}, topic_type is {self._topic_type}, first_step, round={self._round} starts")
        # Add the question to the memory.
        self._history.append(UserMessage(content=message.content, source="user"))
        # Make an inference using the model.
        prompt = "Now given the following math problem, please complete the first step of the math problem."
        model_result = await self._model_client.create(self._system_messages + self._history)
        assert isinstance(model_result.content, str)
        # Add the response to the memory.
        self._history.append(AssistantMessage(content=model_result.content, source=self.metadata["type"]))
        self._logger.log(f"{'-'*80}\nSolver {self.id} round {self._round}:\n{model_result.content}")
        self._logger.log(f"{'-'*80}\nSolver {self.id} round {self._round}:\n{model_result.content}")
        # Extract the answer from the response.
        match = re.search(r"\{\{(\-?\d+(\.\d+)?)\}\}", model_result.content)
        if match is None:
            self._logger.log(f"The model response does not contain the answer. model result is {model_result.content}")
            answer=''
        else:
            answer = match.group(1)
        # Increment the counter.
        self._round += 1
        # Publish intermediate response to the topic associated with this solver.
        self._logger.log(f"mad.py MathSolver{self.id}, topic_type={self._topic_type}, first_step completed. Sending attempt to request critique.")
        requester_str = self.id.type + "/" + self.id.key
        self._logger.log(f"mad.py: MathSolver.first_step(): requester_str={requester_str}")
        await self.publish_message(
            CritiqueRequest(
                content=model_result.content,
                question=message.question,
                answer=answer,
                round=self._round,
                requester=requester_str,
            ),
            topic_id=DefaultTopicId(type=self._topic_type),
        )

# for competition: handles Critique Requests -> generates a critique of another solver's reasoning/answer
    @message_handler
    async def handle_critique_request(self, message: CritiqueRequest, ctx: MessageContext) -> None:
        self._logger.log(f"mad.py: MathSolver.handle_critique_request(): MathSolve id={self.id.type + "/" + self.id.key}; message={message}")
        # 1. Prepare the prompt.
        critique_system_message = SystemMessage(content=("""
Your task is to review a partial solution to a math problem and identify any errors.

# Steps

1. **Understand the Problem**: read and comprehend the math reasoning problem.
2. **Review the Partial Solution**: Check for mistakes in logic or calculation.
3. **Critique**: explain any errors found clearly.

# Output Format

- Provide a concise critique to the partial solution; do not provide the final answer in the response.
- Keep your responses under 100 words.

# Notes

- Focus on accuracy in identifying mistakes
- Ensure your explanation is clear and to the point
- The solution may be incomplete; prioritize accuracy in logic and calculation
                    
"""))
        prompt = "Now given the following math problem and partial solution, please carefully inspect the solution and point out any mistakes."
        prompt += "Math problem: "
        prompt += message.question
        prompt += "Partial solution: "
        prompt += message.content
        
        # 2. Send the prompt to the LLM itself to generate a critique.
        model_result = await self._model_client.create(messages=[critique_system_message, UserMessage(content=prompt, source="user")])
        assert isinstance(model_result.content, str)
        self._logger.log(f"mad.py MathSolver{self.id}, handle_critique_request model_result: {model_result.content}")
        
        # 3. Send the critique to the agent that requested the critique.
        self._logger.log(f"mad.py MathSolver{self.id}, topic_type={self._topic_type}, handle_critique_request completed. Sending critique to requester {message.requester}.")
        await self.send_message(CritiqueResponse(critique = model_result.content, content = message.content, question = message.question, answer = message.answer, round = message.round), AgentId.from_str(message.requester))

# for competition: handles Critique Responses -> receive critique and generate next step
    @message_handler
    async def handle_critique_response(self, message: CritiqueResponse, ctx: MessageContext) -> None:
        self._logger.log(f"MathSolver {self.id}, topic_type is {self._topic_type}, handle_critique_response, round={self._round} starts.")
        # Add neighbor's critique to buffer.
        self._buffer.setdefault(message.round, []).append(message)
        # When all critiques are collected, create prompt and ask LLM
        if len(self._buffer[message.round]) == self._num_neighbors:
            self._logger.log(
                f"{'-'*80}\nSolver {self.id} round {message.round}:\nReceived all critiques from {self._num_neighbors} neighbors."
            )
            self._logger.log(f"{'-'*80}\nSolver {self.id} round {message.round}:\nReceived all critiques from {self._num_neighbors} neighbors.")
            # Prompt preparation
            critique_system_message = SystemMessage(content=("""
Your task is to review a partial solution and its two critiques for a math reasoning problem, correct any
errors, and provide the next correct step in the solution.

# Steps

1. **Understand the problem**: read and interpret the math problem.
2. **Review the partial solution**: identify any mistakes or gaps.
3. **Evaluate the Critique**: assess the critiques' accuracy.
4. **Address the Critique**: replace the partial solution with a corrected solution. If the final answer hasnâ€™t been reached, provide only the next logical step.

# Output format

- Add only one step per response.
- Clearly explain your reasoning.
- If reaching the final answer, use the format: The answer is {{numerical answer}}. For example, if the answer is 42, the final sentence of the response should be "The answer is {{42}}."
- Limit your response to 100 words.

Now given the following math problem, previous steps and critiques, please carefully consider the critique and correct any mistakes as the next step.
"""))
        
            self._history.append(UserMessage(content = ("Math problem: " + message.question), source="user"))
            self._history.append(AssistantMessage(content = "Previous steps (partial solution): " + message.content, source=self.metadata["type"]))
            for resp in self._buffer[message.round]:
                self._history.append(AssistantMessage(content=("A critique: " + message.critique), source=self.metadata["type"]))

            # Send the prompt to the LLM to generative the next step.
            model_result = await self._model_client.create(messages=[critique_system_message] + self._history)
            assert isinstance(model_result.content, str)

            # Add the response to the memory.
            self._history.append(AssistantMessage(content=model_result.content, source=self.metadata["type"]))
            self._logger.log(f"{'-'*80}\nSolver {self.id} round {self._round}:\n{model_result.content}")
            # Extract the answer from the response.
            match = re.search(r"\{\{(\-?\d+(\.\d+)?)\}\}", model_result.content)
            if match is None:
                self._logger.log(f"The model response does not contain the answer. model result is {model_result.content}")
                answer=''
            else:
                answer = match.group(1)
            # Increment the counter.
            self._round += 1
            if self._round > self._max_round:
                self._logger.log(f"Error max_round violation")
            elif self._round == self._max_round:
                # If the counter reaches the maximum round, publishes a final response.
                self._logger.log(f"mad.py MathSolver {self.id}, topic_type={self._topic_type}, handle_critique_response max_round is reached. Publish final answer = {answer}.")
                solver_name="MathSolver"+str(self.id)
                await self.publish_message(FinalSolverResponse(answer=answer, solver=solver_name), topic_id=DefaultTopicId())
            else:
                # Publish intermediate response to the topic associated with this solver.
                self._logger.log(f"mad.py MathSolver{self.id}, topic_type={self._topic_type}, handle_critique_response completed.")
                requester_str = self.id.type + "/" + self.id.key
                await self.publish_message(
                    CritiqueRequest(
                        content=model_result.content,
                        question=message.question,
                        answer=answer,
                        round=self._round,
                        requester=requester_str
                    ),
                    topic_id=DefaultTopicId(type=self._topic_type),
                )

                # Clear buffer.
                self._buffer.pop(message.round)

###################################################################
#  Math Aggregator
###################################################################
@default_subscription
class MathAggregator(RoutedAgent):
    def __init__(self, num_solvers: int, logger, answers) -> None:
        super().__init__("Math Aggregator")
        self._num_solvers = num_solvers
        self._logger = logger
        self._answers = answers
        self._question_index = -1
        self._buffer: List[FinalSolverResponse] = []
        self._logger.log(f"mad.py MathAggregator.init num_solvers={num_solvers}")

    @message_handler
    async def handle_question(self, message: Question, ctx: MessageContext) -> None:
        self._logger.log(f"{'-'*80}\nAggregator {self.id} received question (index={message.index}):\n{message.content}")        
        self._question_index = message.index
        prompt = (
            f"Can you solve the following math problem?\n{message.content}\n"
            "Explain your reasoning. Your final answer should be a single numerical number, "
            "in the form of {{answer}}, at the end of your response."
        )
        self._logger.log(f"{'-'*80}\nAggregator {self.id} publishes initial solver request.")
        await self.publish_message(SolverRequest(content=prompt, question=message.content, do_reset=True), topic_id=DefaultTopicId())

    @message_handler
    async def handle_final_solver_response(self, message: FinalSolverResponse, ctx: MessageContext) -> None:
        self._logger.log(f"mad.py MathAggregator.handle_final_solver_response solver={message.solver}")
        self._buffer.append(message)
        if len(self._buffer) == self._num_solvers:
            self._logger.log(f"{'-'*80}\nAggregator {self.id} received all final answers from {self._num_solvers} solvers.")
            # Find the majority answer.
            answers = [resp.answer for resp in self._buffer]
            majority_answer = max(set(answers), key=answers.count)
            # Publish the aggregated response.
            print(f"mad.py MathAggregator.handle_final_solver_response majority_answer={majority_answer}. Append to answers.")
            self._logger.log(f"mad.py MathAggregator.handle_final_solver_response majority_answer={majority_answer}. Append to answers.")
            if self._question_index>=0:
                self._answers[self._question_index] = majority_answer
            else:
                print(f"mad.py MathAggregator.handle_final_solver_response invalid question index={self._question_index}")
                self._logger.log(f"mad.py MathAggregator.handle_final_solver_response invalid question index={self._question_index}")
            #await self.publish_message(Answer(content=majority_answer), topic_id=DefaultTopicId())
            # Clear the responses.
            self._buffer.clear()
            self._logger.log(f"{'-'*80}\nAggregator {self.id} publishes final answer:\n{majority_answer}")
